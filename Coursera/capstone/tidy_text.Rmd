---
title: "tidy_text"
author: "Laura Daniel"
date: "8/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library("tm"))
suppressMessages(library("dplyr"))
```

## Reading in part of the data - will use this to work with.
```{r read_data, cache=TRUE}

blog <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.blogs.txt", n=10000, skipNul = TRUE)

news <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.news.txt", n=10000, skipNul=TRUE)

twitter <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.twitter.txt", n=10000, skipNul = TRUE)


blog <- as.data.frame(blog)
blog$source <- "blog"
colnames(blog) <- c("text", "source")

news <- as.data.frame(news)
news$source <- "news"
colnames(news) <- c("text", "source")

twitter <-as.data.frame(twitter)
twitter$source <- "twitter"
colnames(twitter) <- c("text", "source")

combined <- bind_rows(blog, news, twitter)
df <- tibble(lines=1:30000, text=combined$text, source=combined$source)
```

## Converting text to a tibble and tokenizing
By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. 
```{r}
library(dplyr)
library(reshape2)
library(tidytext)
library(stringr)

tidy_df <- df %>% unnest_tokens(word, text) %>%
     anti_join(stop_words) %>%
     mutate(word = str_extract(word, "[a-z']+")) %>%
     filter(!str_detect(word, "[0-9]"),
            word != "shit",
            word != "fuck")

count <- tidy_df %>% count(word, sort = TRUE) 

```


Visualize frequent words
```{r}
library(ggplot2)

plot_freq <- tidy_df %>%
     count(word, sort = TRUE) %>%
     filter(n > 400) %>%
     mutate(word = reorder(word, n)) %>%
     ggplot(aes(word, n)) +
     geom_col() +
     xlab(NULL) +
     coord_flip()
plot_freq
```

```{r}
library(tidyr)
library(stringr)

frequency <- tidy_df %>% 
  count(word) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n)
head(frequency, 10)
```


```{r wordcloud}
library(wordcloud)

tidy_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 50))
```

Comparing frequency by source type

```{r}
library(tidyr)

frequency <- tidy_df %>% 
     count(source, word) %>%
     group_by(source) %>%
     mutate(proportion = n / sum(n)) %>% 
     select(-n) %>% 
     spread(source, proportion) %>% 
     gather(source, proportion, c(blog,news))

cor.test(data = frequency[frequency$source == "blog",],
         ~ proportion + `twitter`)
```

```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `twitter`, color = abs(`twitter` - proportion)))  +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Twitter", x = NULL) +
     facet_wrap(~source, ncol = 2)
```
Sentament analysis
```{r}
bing_word_counts <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

Comoparing negative words in each source
```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_df %>%
  group_by(source) %>%
  summarize(words = n())

tidy_df %>%
  semi_join(bingnegative) %>%
  group_by(source) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c('source')) %>%
  mutate(ratio = negativewords/words) %>%
  ungroup()
```

 ## Ngrams
 
```{r bigram}
# https://www.tidytextmining.com/ngrams.html

all_bigrams <- df %>%
     unnest_tokens(bigram, text, token = "ngrams", n = 2)
     
all_bigrams %>% 
     count(bigram, sort=TRUE)
```
 As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” (see [Chapter 1](https://www.tidytextmining.com/tidytext.html#tidytext)). This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.
 
```{r}
library(tidyr)

bigrams_separated <- all_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)



# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

## you might want to bring them back together after removing stop words
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>% count(bigram, sort=TRUE)
```
 
In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:
```{r}
tri<- df %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word)

 tri %>% count(word1, word2, word3, sort = TRUE)
```
A bigram can also be treated as a term in a document in the same way that we treated individual words. For example, we can look at the tf-idf (Chapter 3) of bigrams across Austen novels. These tf-idf values can be visualized within each book, just as we did for words (Figure 4.1).
```{r}
bigram_tf_idf <- bigrams_united %>%
  count(source, bigram) %>%
  bind_tf_idf(bigram, source, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```
Examine the most frequent words that were preceded by “not” and were associated with a sentiment.
```{r}
AFINN <- get_sentiments("afinn")
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words

library(ggplot2)

not_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip()
```

Bring bigrams all together
## Use this function to create bigram from df
```{r creating_function}
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(igraph)
library(ggraph)

count_words <- function(dataset) {
  dataset %>%
    unnest_tokens(word, text, to_lower = TRUE) %>%
    anti_join(stop_words) %>%
    mutate(word = str_extract(word, "[a-z']+")) %>%
    filter(!str_detect(word, "[0-9]"),
           word != "shit",
           word != "fuck") %>%
    count(word, sort = TRUE)
}

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2, to_lower = TRUE) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

count_trigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 3, to_lower = TRUE) %>%
    separate(bigram, c("word1", "word2", "word3"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word,
           !word3 %in% stop_words$word) %>%
    count(word1, word2, word3, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}
```
At this point, we could visualize bigrams in other works.

# the King James version is book 10 on Project Gutenberg:
library(gutenbergr)
kjv <- gutenberg_download(10)
library(stringr)

kjv_bigrams <- kjv %>%
  count_bigrams()

# filter out rare combinations, as well as digits
kjv_bigrams %>%
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()

  
```{r}
# creating list of words


my_gram <- df %>% count_words

# creating the bigram list
my_bigrams <- df %>%
  count_bigrams()

# creating the trigram list
my_trigrams <- df %>%
     count_trigrams()

# filter out rare combinations, as well as digits
my_bigrams %>%
  filter(n > 15,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  visualize_bigrams()

results <- my_bigrams %>% filter(word1=="1")
suggestion <- results[1,2]
suggestion
```

# My preditive function
```{r}
## Use count_bigrams function above to format data

output_word <- function(input_word) {
  suggestion <- my_bigrams %>%
    filter(word1 == input_word)
    suggestion[1,2]
}
output_word("happy")

return_word <- function(woord1, woord2){
     woord <- my_trigrams %>%
          filter_(~word1 == woord1, ~word2 == woord2) %>%
          sample_n(1, weight = n) %>%
          .[["word3"]]
     if(length(woord) == 0){
          bleh <- my_bigrams %>% 
               filter_(~word1 == woord2) %>%
               sample_n(1, weight = n) %>%
               .[["word2"]]
          woord <- bleh
          if(length(woord)==0){
               blehbleh <- stop_words %>%
                    sample_n(1) %>%
               .[["word"]]
               woord <- blehbleh
          }
     }
     woord
}

return_word("happy", "mothers")
```


Creating a function that give the most likely word given a certain word.

One useful function from widyr is the pairwise_count() function. The prefix pairwise_ means it will result in one row for each pair of words in the word variable. This lets us count common pairs of words co-appearing within the same section:
```{r}
library(widyr)

word_cors <- tidy_df %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, source, sort = TRUE)

word_cors %>%
  filter(item1 == "gods") %>%
     filter(correlation > 0.3)

```
## Milestone report on September 16
How can you efficiently store an n-gram model (think Markov Chains)?
How can you use the knowledge about word frequencies to make your model smaller and more efficient?
How many parameters do you need (i.e. how big is n in your n-gram model)?
Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
How do you evaluate whether your model is any good?
How can you use backoff models to estimate the probability of unobserved n-grams?