---
title: "tidy_text"
author: "Laura Daniel"
date: "8/23/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library("tm"))
suppressMessages(library("dplyr"))
```

## Reading in part of the data - will use this to work with.
```{r}

blog <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.blogs.txt", n=1000, skipNul = TRUE)

news <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.news.txt", n=1000, skipNul=TRUE)

twitter <- readLines("/Users/daniell2/Desktop/Data_Specialization/R_Programming/capstone/final/en_US/en_US.twitter.txt", n=1000, skipNul = TRUE)


blog <- as.data.frame(blog)
blog$source <- "blog"
colnames(blog) <- c("text", "source")

news <- as.data.frame(news)
news$source <- "news"
colnames(news) <- c("text", "source")

twitter <-as.data.frame(twitter)
twitter$source <- "twitter"
colnames(twitter) <- c("text", "source")

combined <- bind_rows(blog, news, twitter)
df <- tibble(lines=1:3000, text=combined$text, source=combined$source)
```

## Converting text to a tibble and tokenizing
By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. 
```{r}
library(dplyr)
library(reshape2)
library(tidytext)

tidy_df <- df %>% unnest_tokens(word, text) %>%
     anti_join(stop_words) %>%
     mutate(word = str_extract(word, "[a-z']+")) %>%
     filter(!str_detect(word, "[0-9]"),
            word != "shit",
            word != "fuck")
tidy_df


count <- tidy_df %>% count(word, sort = TRUE) 

```


Visualize frequent words
```{r}
library(ggplot2)

plot_freq <- tidy_df %>%
     count(word, sort = TRUE) %>%
     filter(n > 50) %>%
     mutate(word = reorder(word, n)) %>%
     ggplot(aes(word, n)) +
     geom_col() +
     xlab(NULL) +
     coord_flip()
plot_freq
```

```{r}
library(tidyr)
library(stringr)

frequency <- tidy_df %>% 
  count(word) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n)
head(frequency, 100)
```

```{r wordcloud}
library(wordcloud)

tidy_df %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
comparing frequency by source type

```{r}
library(tidyr)

frequency <- tidy_df %>% 
     count(source, word) %>%
     group_by(source) %>%
     mutate(proportion = n / sum(n)) %>% 
     select(-n) %>% 
     spread(source, proportion) %>% 
     gather(source, proportion, c(blog,news))

cor.test(data = frequency[frequency$source == "blog",],
         ~ proportion + `twitter`)
```

```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `twitter`, color = abs(`twitter` - proportion)))  +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Twitter", x = NULL) +
     facet_wrap(~source, ncol = 2)
```
Sentament analysis
```{r}
bing_word_counts <- tidy_df %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts

bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```
Comoparing negative words in each source
```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_df %>%
  group_by(source) %>%
  summarize(words = n())

tidy_df %>%
  semi_join(bingnegative) %>%
  group_by(source) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c('source')) %>%
  mutate(ratio = negativewords/words) %>%
  ungroup()
```

 ## Ngrams
 
```{r bigram}
# https://www.tidytextmining.com/ngrams.html

all_bigrams <- df %>%
     unnest_tokens(bigram, text, token = "ngrams", n = 2)
     
all_bigrams %>% 
     count(bigram, sort=TRUE)
```
 As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” (see [Chapter 1](https://www.tidytextmining.com/tidytext.html#tidytext)). This is a useful time to use tidyr’s separate(), which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.
 
```{r}
library(tidyr)

bigrams_separated <- all_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

## you might want to bring them back together after removing stop words
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united %>% count(bigram, sort=TRUE)
```
 
In other analyses you may be interested in the most common trigrams, which are consecutive sequences of 3 words. We can find this by setting n = 3:
```{r}
tri<- df %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word)

 tri %>% count(word1, word2, word3, sort = TRUE)
```
A bigram can also be treated as a term in a document in the same way that we treated individual words. For example, we can look at the tf-idf (Chapter 3) of bigrams across Austen novels. These tf-idf values can be visualized within each book, just as we did for words (Figure 4.1).
```{r}
bigram_tf_idf <- bigrams_united %>%
  count(source, bigram) %>%
  bind_tf_idf(bigram, source, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```
Examine the most frequent words that were preceded by “not” and were associated with a sentiment.
```{r}
AFINN <- get_sentiments("afinn")1
not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

not_words
```